{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yashgt/AI101/blob/main/TRAINING_LLAMA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHsssBgWM_l0"
      },
      "source": [
        "# The Role Teller"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install following libraries:\n",
        "trl, bitsandbytes\n"
      ],
      "metadata": {
        "id": "_Ehx9D2yzU65"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-yikV8pRBer9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, set_seed, BitsAndBytesConfig\n",
        "from datasets import load_dataset, Dataset, DatasetDict\n",
        "import wandb\n",
        "from peft import LoraConfig\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uuTX-xonNeOK"
      },
      "outputs": [],
      "source": [
        "# Constants\n",
        "\n",
        "BASE_MODEL = \"meta-llama/Llama-3.2-3B\"\n",
        "PROJECT_NAME = \"roleteller\"\n",
        "HF_USER = \"yash-ganthe\" # your HF name here!\n",
        "\n",
        "#DATA_USER = \"yash-ganthe\"\n",
        "#DATASET_NAME = f\"{DATA_USER}/training_data\"\n",
        "\n",
        "RUN_NAME =  f\"{datetime.now():%Y-%m-%d_%H.%M.%S}\"\n",
        "#RUN_NAME = f\"2026-01-11_06.45.13\"\n",
        "PROJECT_RUN_NAME = f\"{PROJECT_NAME}-{RUN_NAME}\"\n",
        "HUB_MODEL_NAME = f\"{HF_USER}/{PROJECT_RUN_NAME}\"\n",
        "\n",
        "\n",
        "# Hyper-parameters - overall\n",
        "\n",
        "EPOCHS = 100\n",
        "BATCH_SIZE = 32\n",
        "MAX_SEQUENCE_LENGTH = 128\n",
        "GRADIENT_ACCUMULATION_STEPS = 1\n",
        "\n",
        "# Hyper-parameters - QLoRA\n",
        "\n",
        "QUANT_4_BIT = True\n",
        "LORA_R = 32\n",
        "LORA_ALPHA = LORA_R * 2\n",
        "ATTENTION_LAYERS = [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]\n",
        "MLP_LAYERS = [\"gate_proj\", \"up_proj\", \"down_proj\"]\n",
        "TARGET_MODULES = ATTENTION_LAYERS\n",
        "LORA_DROPOUT = 0.1\n",
        "\n",
        "# Hyper-parameters - training\n",
        "\n",
        "LEARNING_RATE = 1e-4\n",
        "WARMUP_RATIO = 0.01\n",
        "LR_SCHEDULER_TYPE = 'cosine'\n",
        "WEIGHT_DECAY = 0.001\n",
        "OPTIMIZER = \"paged_adamw_32bit\"\n",
        "\n",
        "capability = torch.cuda.get_device_capability()\n",
        "use_bf16 = capability[0] >= 8\n",
        "\n",
        "# Tracking\n",
        "\n",
        "VAL_SIZE = 500\n",
        "LOG_STEPS = 5\n",
        "SAVE_STEPS = 100\n",
        "# LOG_TO_WANDB = True"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# A100 GPU supports this; T4 does not natively\n",
        "use_bf16 = True\n",
        "use_bf16\n"
      ],
      "metadata": {
        "id": "-DUwSPbE22M1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# More on Optimizers\n",
        "\n",
        "https://huggingface.co/docs/transformers/main/en/perf_train_gpu_one#optimizers\n",
        "\n",
        "The most common is Adam or AdamW (Adam with Weight Decay).  \n",
        "Adam achieves good convergence by storing the rolling average of the previous gradients; however, it adds an additional memory footprint of the order of the number of model parameters.\n"
      ],
      "metadata": {
        "id": "PfvyitbgCMMQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JArT3QAQAjx"
      },
      "source": [
        "### Log in to HuggingFace and Weights & Biases\n",
        "\n",
        "If you don't already have a HuggingFace account, visit https://huggingface.co to sign up and create a token.\n",
        "\n",
        "Then select the Secrets for this Notebook by clicking on the key icon in the left, and add a new secret called `HF_TOKEN` with the value as your token.\n",
        "\n",
        "Repeat this for weightsandbiases at https://wandb.ai and add a secret called `WANDB_API_KEY`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WyFPZeMcM88v"
      },
      "outputs": [],
      "source": [
        "# Log in to HuggingFace\n",
        "\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "login(hf_token, add_to_git_credential=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_data = [{\"name\":\"Yuvraj Chhabra\", \"role\": \"Project Manager\"}, {\"name\" : \"Yash Ganthe\", \"role\":\"Architect\"}, {\"name\" : \"Vaibhav Gandhi\", \"role\": \"Architect\"}, {\"name\":\"Rahul Hegde\", \"role\": \"Solution Architect\"}, {\"name\" : \"Shyamal Dhole\", \"role\":\"Dev Lead\"}, {\"name\" : \"Vineet Vyas\", \"role\": \"Specialist\"}, {\"name\":\"Michael John\", \"role\": \"DBA\"}, {\"name\" : \"Neha Magarde\", \"role\":\"Dev Lead\"}, {\"name\" : \"Sonali Patil\", \"role\": \"QA Lead\"}, {\"name\" : \"Rohini Golatkar\", \"role\": \"QA Lead\"}  ]"
      ],
      "metadata": {
        "id": "8pGS7QpUEgLY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_datalist = []\n",
        "for item in training_data:\n",
        "  train_datalist.append({'prompt': \"What is the role of \"+ item['name'] + \"?\", 'completion': item['role']})\n",
        "train_dataset = Dataset.from_list(train_datalist)\n",
        "train_datalist"
      ],
      "metadata": {
        "id": "zFk-OspoE0rk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJWQ0a3wZ0Bw"
      },
      "source": [
        "## Now load the Tokenizer and Model\n",
        "\n",
        "The model is \"quantized\" - we are reducing the precision to 4 bits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9lb7M9xn46wx"
      },
      "outputs": [],
      "source": [
        "# pick the right quantization\n",
        "\n",
        "if QUANT_4_BIT:\n",
        "  quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16 if use_bf16 else torch.float16,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        "  )\n",
        "else:\n",
        "  quant_config = BitsAndBytesConfig(\n",
        "    load_in_8bit=True,\n",
        "    bnb_8bit_compute_dtype=torch.bfloat16 if use_bf16 else torch.float16,\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_O04fKxMMT-"
      },
      "outputs": [],
      "source": [
        "# Load the Tokenizer and the Model\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL,\n",
        "    quantization_config=quant_config,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "base_model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "print(f\"Memory footprint: {base_model.get_memory_footprint() / 1e6:.1f} MB\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_model"
      ],
      "metadata": {
        "id": "l8D0Nvxvi9-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TRAINING\n",
        "\n",
        "## We set up the configuration for Training\n",
        "\n",
        "We need to create 2 objects:\n",
        "\n",
        "A LoraConfig object with our hyperparameters for LoRA\n",
        "\n",
        "An SFTConfig with our overall Training parameters"
      ],
      "metadata": {
        "id": "4DaOeBhyy9eS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fCwmDmkSATvj"
      },
      "outputs": [],
      "source": [
        "# LoRA Parameters\n",
        "\n",
        "lora_parameters = LoraConfig(\n",
        "    lora_alpha=LORA_ALPHA,\n",
        "    lora_dropout=LORA_DROPOUT,\n",
        "    r=LORA_R,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=TARGET_MODULES,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training parameters\n",
        "\n",
        "train_parameters = SFTConfig(\n",
        "    output_dir=PROJECT_RUN_NAME,\n",
        "    num_train_epochs=EPOCHS,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=1,\n",
        "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
        "    optim=OPTIMIZER,\n",
        "    save_steps=SAVE_STEPS,\n",
        "    save_total_limit=10,\n",
        "    logging_steps=LOG_STEPS,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    weight_decay=0.001,\n",
        "    fp16=not use_bf16,\n",
        "    #fp16=use_bf16,\n",
        "    bf16=use_bf16,\n",
        "    max_grad_norm=0.3,\n",
        "    max_steps=-1,\n",
        "    warmup_ratio=WARMUP_RATIO,\n",
        "    group_by_length=True,\n",
        "    lr_scheduler_type=LR_SCHEDULER_TYPE,\n",
        "    # report_to=\"wandb\" if LOG_TO_WANDB else None,\n",
        "    run_name=RUN_NAME,\n",
        "    max_length=MAX_SEQUENCE_LENGTH,\n",
        "    save_strategy=\"steps\",\n",
        "    hub_strategy=\"every_save\",\n",
        "    push_to_hub=True,\n",
        "    hub_model_id=HUB_MODEL_NAME,\n",
        "    hub_private_repo=True,\n",
        "    eval_strategy= \"no\",\n",
        "    eval_steps=SAVE_STEPS\n",
        ")"
      ],
      "metadata": {
        "id": "EYxQi6y91xtY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AND NOW - create the trainer"
      ],
      "metadata": {
        "id": "QwPwRYUs122a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fine_tuning = SFTTrainer(\n",
        "    model=base_model,\n",
        "    train_dataset=train_dataset,\n",
        "    #eval_dataset=val,\n",
        "    peft_config=lora_parameters,\n",
        "    args=train_parameters\n",
        ")"
      ],
      "metadata": {
        "id": "s87Tl3Nk1hbI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## In the next cell, we kick off fine-tuning!\n",
        "\n",
        "This will run for some time, uploading to the hub every SAVE_STEPS steps.\n",
        "\n",
        "After some time, Google might stop your colab. For people on free plans, it can happen whenever Google is low on resources. For anyone on paid plans, they can give you up to 24 hours, but there's no guarantee.\n",
        "\n",
        "If your server is stopped, you can follow my colab here to resume from your last save:\n",
        "\n",
        "https://colab.research.google.com/drive/1qGTDVIas_Vwoby4UVi2vwsU0tHXy8OMO#scrollTo=R_O04fKxMMT-\n",
        "\n",
        "I've saved this colab with my final run in the output so you can see the example. The trick is that I needed to set `is_trainable=True` when loading the fine_tuned model.\n",
        "\n",
        "### Anyway, with that in mind, let's kick this off!"
      ],
      "metadata": {
        "id": "ArjP7_OCQOin"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine-tune!\n",
        "fine_tuning.train()\n",
        "\n",
        "# Push our fine-tuned model to Hugging Face\n",
        "fine_tuning.model.push_to_hub(PROJECT_RUN_NAME, private=True)\n",
        "print(f\"Saved to the hub: {PROJECT_RUN_NAME}\")"
      ],
      "metadata": {
        "id": "GfvAxnXPvB7w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import PeftModel  # PARAMETER EFFICIENT FINE TUNING\n",
        "#HUB_MODEL_NAME = f\"yash-ganthe/roleteller-2025-12-18_08.03.04\"\n",
        "fine_tuned_model = PeftModel.from_pretrained(base_model, HUB_MODEL_NAME)"
      ],
      "metadata": {
        "id": "AQBpDqySpEsH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fine_tuned_model"
      ],
      "metadata": {
        "collapsed": true,
        "id": "AlX2ofrLw0ma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Memory footprint: {fine_tuned_model.get_memory_footprint() / 1e6:.1f} MB\")"
      ],
      "metadata": {
        "id": "2pI36Q2R6Cf9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Use Transformers**"
      ],
      "metadata": {
        "id": "df9mtYHHqQIu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "question = \"What is the role of Yuvraj Chhabra?\"\n",
        "generator = pipeline(\"text-generation\", model=HUB_MODEL_NAME, device=\"cuda\")\n",
        "output = generator(question, max_new_tokens=128, return_full_text=False)[0]\n",
        "print(output[\"generated_text\"])"
      ],
      "metadata": {
        "id": "Cv2MqDJcx3Jc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenizer**"
      ],
      "metadata": {
        "id": "ftxpMUpeqwRs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\""
      ],
      "metadata": {
        "id": "SJXYaH26r3Xo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer(\"What is the role of Yuvraj Chhabra?\", return_tensors=\"pt\").to(\"cuda\")\n",
        "inputs"
      ],
      "metadata": {
        "id": "3lJUdvVTq5rV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = fine_tuned_model.generate(**inputs, max_new_tokens=80)\n"
      ],
      "metadata": {
        "id": "nPS2Bh3a45oV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs"
      ],
      "metadata": {
        "id": "2Cjgp89Is0yz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.decode(outputs[0]))"
      ],
      "metadata": {
        "id": "_ALBLtmLs3Bo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "#del fine_tuned_model, inputs, tokenizer, outputs\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "M4hB2C7w2yEg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LhX1QZET1V4m"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}