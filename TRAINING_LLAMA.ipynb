{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yashgt/AI101/blob/main/TRAINING_LLAMA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHsssBgWM_l0"
      },
      "source": [
        "# The Price Is Right - Week 7\n",
        "\n",
        "## Day 3 and Day 4: Training!\n",
        "\n",
        "This is what it's all been about!\n",
        "\n",
        "If you are using LITE_MODE=True, then please run this on a free T4 box.\n",
        "\n",
        "If you are using LITE_MODE-False, then please use a paid A100 with high memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MDyR63OTNUJ6"
      },
      "outputs": [],
      "source": [
        "!pip install -q --upgrade bitsandbytes==0.48.2 trl==0.25.1\n",
        "!wget -q https://raw.githubusercontent.com/ed-donner/llm_engineering/main/week7/util.py -O util.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-yikV8pRBer9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, set_seed, BitsAndBytesConfig\n",
        "from datasets import load_dataset, Dataset, DatasetDict\n",
        "import wandb\n",
        "from peft import LoraConfig\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uuTX-xonNeOK"
      },
      "outputs": [],
      "source": [
        "# Constants\n",
        "\n",
        "BASE_MODEL = \"meta-llama/Llama-3.2-3B\"\n",
        "PROJECT_NAME = \"price\"\n",
        "HF_USER = \"ed-donner\" # your HF name here!\n",
        "\n",
        "LITE_MODE = True\n",
        "\n",
        "DATA_USER = \"ed-donner\"\n",
        "DATASET_NAME = f\"{DATA_USER}/items_prompts_lite\" if LITE_MODE else f\"{DATA_USER}/items_prompts_full\"\n",
        "\n",
        "RUN_NAME =  f\"{datetime.now():%Y-%m-%d_%H.%M.%S}\"\n",
        "if LITE_MODE:\n",
        "  RUN_NAME += \"-lite\"\n",
        "PROJECT_RUN_NAME = f\"{PROJECT_NAME}-{RUN_NAME}\"\n",
        "HUB_MODEL_NAME = f\"{HF_USER}/{PROJECT_RUN_NAME}\"\n",
        "\n",
        "# Hyper-parameters - overall\n",
        "\n",
        "EPOCHS = 1 if LITE_MODE else 3\n",
        "BATCH_SIZE = 32 if LITE_MODE else 256\n",
        "MAX_SEQUENCE_LENGTH = 128\n",
        "GRADIENT_ACCUMULATION_STEPS = 1\n",
        "\n",
        "# Hyper-parameters - QLoRA\n",
        "\n",
        "QUANT_4_BIT = True\n",
        "LORA_R = 32 if LITE_MODE else 256\n",
        "LORA_ALPHA = LORA_R * 2\n",
        "ATTENTION_LAYERS = [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]\n",
        "MLP_LAYERS = [\"gate_proj\", \"up_proj\", \"down_proj\"]\n",
        "TARGET_MODULES = ATTENTION_LAYERS if LITE_MODE else ATTENTION_LAYERS + MLP_LAYERS\n",
        "LORA_DROPOUT = 0.1\n",
        "\n",
        "# Hyper-parameters - training\n",
        "\n",
        "LEARNING_RATE = 1e-4\n",
        "WARMUP_RATIO = 0.01\n",
        "LR_SCHEDULER_TYPE = 'cosine'\n",
        "WEIGHT_DECAY = 0.001\n",
        "OPTIMIZER = \"paged_adamw_32bit\"\n",
        "\n",
        "capability = torch.cuda.get_device_capability()\n",
        "use_bf16 = capability[0] >= 8\n",
        "\n",
        "# Tracking\n",
        "\n",
        "VAL_SIZE = 500 if LITE_MODE else 1000\n",
        "LOG_STEPS = 5 if LITE_MODE else 10\n",
        "SAVE_STEPS = 100 if LITE_MODE else 200\n",
        "LOG_TO_WANDB = True"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# A100 GPU supports this; T4 does not natively\n",
        "\n",
        "use_bf16"
      ],
      "metadata": {
        "id": "-DUwSPbE22M1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# More on Optimizers\n",
        "\n",
        "https://huggingface.co/docs/transformers/main/en/perf_train_gpu_one#optimizers\n",
        "\n",
        "The most common is Adam or AdamW (Adam with Weight Decay).  \n",
        "Adam achieves good convergence by storing the rolling average of the previous gradients; however, it adds an additional memory footprint of the order of the number of model parameters.\n"
      ],
      "metadata": {
        "id": "PfvyitbgCMMQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JArT3QAQAjx"
      },
      "source": [
        "### Log in to HuggingFace and Weights & Biases\n",
        "\n",
        "If you don't already have a HuggingFace account, visit https://huggingface.co to sign up and create a token.\n",
        "\n",
        "Then select the Secrets for this Notebook by clicking on the key icon in the left, and add a new secret called `HF_TOKEN` with the value as your token.\n",
        "\n",
        "Repeat this for weightsandbiases at https://wandb.ai and add a secret called `WANDB_API_KEY`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WyFPZeMcM88v"
      },
      "outputs": [],
      "source": [
        "# Log in to HuggingFace\n",
        "\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "login(hf_token, add_to_git_credential=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wvbQ3CtHb3YH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yJNOv3cVvJ68"
      },
      "outputs": [],
      "source": [
        "# Log in to Weights & Biases\n",
        "wandb_api_key = userdata.get('WANDB_API_KEY')\n",
        "os.environ[\"WANDB_API_KEY\"] = wandb_api_key\n",
        "wandb.login()\n",
        "\n",
        "# Configure Weights & Biases to record against our project\n",
        "os.environ[\"WANDB_PROJECT\"] = PROJECT_NAME\n",
        "os.environ[\"WANDB_LOG_MODEL\"] = \"false\"\n",
        "os.environ[\"WANDB_WATCH\"] = \"false\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cvXVoJH8LS6u"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(DATASET_NAME)\n",
        "train = dataset['train']\n",
        "val = dataset['val'].select(range(VAL_SIZE))\n",
        "test = dataset['test']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# if you wish to reduce the training dataset to 10,000 points instead, then uncomment this line:\n",
        "\n",
        "# train = train.select(range(10000))"
      ],
      "metadata": {
        "id": "rJb9IDVjOAn9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_SUsKqA23Gc"
      },
      "outputs": [],
      "source": [
        "if LOG_TO_WANDB:\n",
        "  wandb.init(project=PROJECT_NAME, name=RUN_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJWQ0a3wZ0Bw"
      },
      "source": [
        "## Now load the Tokenizer and Model\n",
        "\n",
        "The model is \"quantized\" - we are reducing the precision to 4 bits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9lb7M9xn46wx"
      },
      "outputs": [],
      "source": [
        "# pick the right quantization\n",
        "\n",
        "if QUANT_4_BIT:\n",
        "  quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16 if use_bf16 else torch.float16,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        "  )\n",
        "else:\n",
        "  quant_config = BitsAndBytesConfig(\n",
        "    load_in_8bit=True,\n",
        "    bnb_8bit_compute_dtype=torch.bfloat16 if use_bf16 else torch.float16,\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_O04fKxMMT-"
      },
      "outputs": [],
      "source": [
        "# Load the Tokenizer and the Model\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL,\n",
        "    quantization_config=quant_config,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "base_model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "print(f\"Memory footprint: {base_model.get_memory_footprint() / 1e6:.1f} MB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AND NOW\n",
        "\n",
        "## We set up the configuration for Training\n",
        "\n",
        "We need to create 2 objects:\n",
        "\n",
        "A LoraConfig object with our hyperparameters for LoRA\n",
        "\n",
        "An SFTConfig with our overall Training parameters"
      ],
      "metadata": {
        "id": "4DaOeBhyy9eS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fCwmDmkSATvj"
      },
      "outputs": [],
      "source": [
        "# LoRA Parameters\n",
        "\n",
        "lora_parameters = LoraConfig(\n",
        "    lora_alpha=LORA_ALPHA,\n",
        "    lora_dropout=LORA_DROPOUT,\n",
        "    r=LORA_R,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=TARGET_MODULES,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training parameters\n",
        "\n",
        "train_parameters = SFTConfig(\n",
        "    output_dir=PROJECT_RUN_NAME,\n",
        "    num_train_epochs=EPOCHS,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=1,\n",
        "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
        "    optim=OPTIMIZER,\n",
        "    save_steps=SAVE_STEPS,\n",
        "    save_total_limit=10,\n",
        "    logging_steps=LOG_STEPS,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    weight_decay=0.001,\n",
        "    fp16=not use_bf16,\n",
        "    bf16=use_bf16,\n",
        "    max_grad_norm=0.3,\n",
        "    max_steps=-1,\n",
        "    warmup_ratio=WARMUP_RATIO,\n",
        "    group_by_length=True,\n",
        "    lr_scheduler_type=LR_SCHEDULER_TYPE,\n",
        "    report_to=\"wandb\" if LOG_TO_WANDB else None,\n",
        "    run_name=RUN_NAME,\n",
        "    max_length=MAX_SEQUENCE_LENGTH,\n",
        "    save_strategy=\"steps\",\n",
        "    hub_strategy=\"every_save\",\n",
        "    push_to_hub=True,\n",
        "    hub_model_id=HUB_MODEL_NAME,\n",
        "    hub_private_repo=True,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=SAVE_STEPS\n",
        ")"
      ],
      "metadata": {
        "id": "EYxQi6y91xtY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AND NOW - create the trainer"
      ],
      "metadata": {
        "id": "QwPwRYUs122a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fine_tuning = SFTTrainer(\n",
        "    model=base_model,\n",
        "    train_dataset=train,\n",
        "    eval_dataset=val,\n",
        "    peft_config=lora_parameters,\n",
        "    args=train_parameters\n",
        ")"
      ],
      "metadata": {
        "id": "s87Tl3Nk1hbI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## In the next cell, we kick off fine-tuning!\n",
        "\n",
        "This will run for some time, uploading to the hub every SAVE_STEPS steps.\n",
        "\n",
        "After some time, Google might stop your colab. For people on free plans, it can happen whenever Google is low on resources. For anyone on paid plans, they can give you up to 24 hours, but there's no guarantee.\n",
        "\n",
        "If your server is stopped, you can follow my colab here to resume from your last save:\n",
        "\n",
        "https://colab.research.google.com/drive/1qGTDVIas_Vwoby4UVi2vwsU0tHXy8OMO#scrollTo=R_O04fKxMMT-\n",
        "\n",
        "I've saved this colab with my final run in the output so you can see the example. The trick is that I needed to set `is_trainable=True` when loading the fine_tuned model.\n",
        "\n",
        "### Anyway, with that in mind, let's kick this off!"
      ],
      "metadata": {
        "id": "ArjP7_OCQOin"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine-tune!\n",
        "fine_tuning.train()\n",
        "\n",
        "# Push our fine-tuned model to Hugging Face\n",
        "fine_tuning.model.push_to_hub(PROJECT_RUN_NAME, private=True)\n",
        "print(f\"Saved to the hub: {PROJECT_RUN_NAME}\")"
      ],
      "metadata": {
        "id": "GfvAxnXPvB7w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "32vvrYRVAUNg"
      },
      "outputs": [],
      "source": [
        "if LOG_TO_WANDB:\n",
        "  wandb.finish()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}