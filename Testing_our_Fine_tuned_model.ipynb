{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yashgt/AI101/blob/main/Testing_our_Fine_tuned_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Price Is Right\n",
        "\n",
        "### And now, to evaluate our fine-tuned open source model\n",
        "\n"
      ],
      "metadata": {
        "id": "GHsssBgWM_l0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q --upgrade bitsandbytes trl\n",
        "!wget -q https://raw.githubusercontent.com/ed-donner/llm_engineering/main/week7/util.py -O util.py"
      ],
      "metadata": {
        "id": "MDyR63OTNUJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# imports\n",
        "\n",
        "import os\n",
        "import re\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, set_seed\n",
        "from datasets import load_dataset, Dataset, DatasetDict\n",
        "from datetime import datetime\n",
        "from peft import PeftModel\n",
        "from util import evaluate"
      ],
      "metadata": {
        "id": "-yikV8pRBer9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Constants\n",
        "\n",
        "BASE_MODEL = \"meta-llama/Llama-3.2-3B\"\n",
        "PROJECT_NAME = \"price\"\n",
        "HF_USER = \"ed-donner\" # your HF name here!\n",
        "\n",
        "LITE_MODE = False\n",
        "\n",
        "DATA_USER = \"ed-donner\"\n",
        "DATASET_NAME = f\"{DATA_USER}/items_prompts_lite\" if LITE_MODE else f\"{DATA_USER}/items_prompts_full\"\n",
        "\n",
        "if LITE_MODE:\n",
        "  RUN_NAME = \"2025-11-30_15.10.55-lite\"\n",
        "  REVISION = None\n",
        "else:\n",
        "  RUN_NAME = \"2025-11-28_18.47.07\"\n",
        "  REVISION = \"b19c8bfea3b6ff62237fbb0a8da9779fc12cefbd\"\n",
        "\n",
        "PROJECT_RUN_NAME = f\"{PROJECT_NAME}-{RUN_NAME}\"\n",
        "HUB_MODEL_NAME = f\"{HF_USER}/{PROJECT_RUN_NAME}\"\n",
        "\n",
        "\n",
        "# Hyper-parameters - QLoRA\n",
        "\n",
        "QUANT_4_BIT = True\n",
        "capability = torch.cuda.get_device_capability()\n",
        "use_bf16 = capability[0] >= 8"
      ],
      "metadata": {
        "id": "uuTX-xonNeOK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Log in to HuggingFace\n",
        "\n",
        "If you don't already have a HuggingFace account, visit https://huggingface.co to sign up and create a token.\n",
        "\n",
        "Then select the Secrets for this Notebook by clicking on the key icon in the left, and add a new secret called `HF_TOKEN` with the value as your token.\n"
      ],
      "metadata": {
        "id": "8JArT3QAQAjx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Log in to HuggingFace\n",
        "\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "login(hf_token, add_to_git_credential=True)"
      ],
      "metadata": {
        "id": "WyFPZeMcM88v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(DATASET_NAME)\n",
        "test = dataset['test']"
      ],
      "metadata": {
        "id": "cvXVoJH8LS6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test[0]"
      ],
      "metadata": {
        "id": "xb86e__Wc7j_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now load the Tokenizer and Model"
      ],
      "metadata": {
        "id": "qJWQ0a3wZ0Bw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pick the right quantization\n",
        "\n",
        "if QUANT_4_BIT:\n",
        "  quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16 if use_bf16 else torch.float16,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        "  )\n",
        "else:\n",
        "  quant_config = BitsAndBytesConfig(\n",
        "    load_in_8bit=True,\n",
        "    bnb_8bit_compute_dtype=torch.bfloat16 if use_bf16 else torch.float16,\n",
        "  )"
      ],
      "metadata": {
        "id": "lAUAAcEC6ido"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Tokenizer and the Model\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL,\n",
        "    quantization_config=quant_config,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "base_model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "# Load the fine-tuned model with PEFT\n",
        "if REVISION:\n",
        "  fine_tuned_model = PeftModel.from_pretrained(base_model, HUB_MODEL_NAME, revision=REVISION)\n",
        "else:\n",
        "  fine_tuned_model = PeftModel.from_pretrained(base_model, HUB_MODEL_NAME)\n",
        "\n",
        "\n",
        "print(f\"Memory footprint: {fine_tuned_model.get_memory_footprint() / 1e6:.1f} MB\")"
      ],
      "metadata": {
        "id": "R_O04fKxMMT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fine_tuned_model"
      ],
      "metadata": {
        "id": "kD-GJtbrdd5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# THE MOMENT OF TRUTH!\n",
        "\n",
        "## Use the model in inference mode\n",
        "\n",
        "Trying to beat \"human\" level of performance at $87.62\n",
        "\n",
        "Or possibly close to gpt-4.1-nano at $62.51\n",
        "\n",
        "## Caveat\n",
        "\n",
        "Keep in mind that prices of goods vary considerably; the model can't predict things like sale prices that it doesn't have any information about."
      ],
      "metadata": {
        "id": "UObo1-RqaNnT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model_predict(item):\n",
        "    inputs = tokenizer(item[\"prompt\"],return_tensors=\"pt\").to(\"cuda\")\n",
        "    with torch.no_grad():\n",
        "        output_ids = fine_tuned_model.generate(**inputs, max_new_tokens=8)\n",
        "    prompt_len = inputs[\"input_ids\"].shape[1]\n",
        "    generated_ids = output_ids[0, prompt_len:]\n",
        "    return tokenizer.decode(generated_ids)"
      ],
      "metadata": {
        "id": "Oj_PzpdFAIMk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(42)\n",
        "evaluate(model_predict, test)"
      ],
      "metadata": {
        "id": "W_KcLvyt6kbb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xI19klarB9wU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}